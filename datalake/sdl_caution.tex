\documentclass[a4paper,12pt]{article}
\usepackage{lettrine}
\begin{document}
\title{The security data lake, a cautionnary tale}
\author{Jeremie Banier, Adriaan Dens}
\date{\today}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Introduction}
This paper was written because we rarely document failure and we even more rarely document 
catastrophic failures. This is a non exhaustive list of things that failed during a project aimed 
to provide a search solution to address some of the shortcommings of the previous gen SIEM.
It also try to highlight the need for a reference architecture that is specific to the specific needs of the infosec community.

The goal of the project was noble and ambitious, the SIEM that supports our SOC could not be used for Hunting (too slow) 
It had the classic fault of only processing some cherry picked events from any given stream, had a short memory problem and it was nearly impossible 
to run IOC over previous event since the correlation only happens on the *real time* stream off events.

In short it was time for a make over ... 

The new setup would have to allow every sources to be fully search-able and also to enrich logs and event in case of missing data
like extracting the user name from one sources and stiching it to other logs from the same IP.
It would also allow for point in time parser upgrade (replaying a new  version of a parser on older logs) as well as having flexible and 
generic parsers. All that while allowing a one year retention period for all logs ...
\newpage
\section{Vision}
\subsection{Pre requisite}
People, full time admin, network guy ...
\subsection{log shipping}
log4j, windows logs, syslog, nifi, logstash, beats ...
container/vm located close to the source
\subsection{Parsing A.K.A E.T.L}
\lettrine{E.T.L}{ stands for} Extract Transform and Load, usually it address the questions of:
\begin{enumerate}
	\item How do you ship the logs to your system.
	\item How do you parse and extract the fields that have values in the log; like ip addresses, username, ...
	\item How Transform them in a data structure that can be used by the others components of your system (json, parquet, ORC, avro, ...)
\end{enumerate}
Shipping the logs in this context means making them accessible in a reliable and preferably fast system to your 'parsers', one common way to do that is to use a 
message broker of some sort to act as a queue in which the logs wait in line to be parsed.
Several options are available but Kafka is the de facto standard in the Apache world.
The advantages are amonsgt other that Kafka is resilient (survive a node crash or reboot), can scale up very easilly (just add more node) and is reliable (we don't want to loose logs) as a bonus 
it support point in time resume and the capacity of the queue (called topics) can be set either in terms of storage or time, which very nicely allow for easy data replay that is extremely handy when you want to validate a new parser.

Now that the data is shipped, we need a system that can scale up lineraly very much like Kafka, we have several options at our disposal depending on the complexity of the parsing, for logs which are CSV or assimilated Apache-Nifi can be a easy to deploy and easy to run solution,
for more complex parsing Apache Storm or Apache Spark represent an excellent alternative and allow you to use either Java, Scala or Python to write your parsers.

EXAMPLE OF STREAMING CODE

One important thing to understand is that all those parser are online or in streaming mode, they work on (close to) real time data...
Also introduce deps on data model
\subsection{storage}
String raw, either as text or as a part of a parsed event. think of future feature extraction, update in parsers...
\newpage
\section{Expected problems versus Unexpected problems}
\subsection{Kerberos}
\lettrine{A}{uthentication} is important, right ? well Kerberos is one hell of a piece of crap.
It may be secure but ho boy, it's not easy to figure it out, luckily some guy put together a guide with everything you need to know about that sucker.
\newpage
\section{Stack}
Which vendor to choose ? do we roll our own ? 
What about gdpr ? encryption at rest, in flight, auditing ? 
\subsection{hadoop versus splunk}
\subsection{horton works, cloudera \& all}
\newpage
\section{Capacity planning}
Self hosted versus cloud, ssd and spinning disks.
\subsection{monitoring}
for failure, capacity and performances.
graphs metrics, log centralisation
\newpage
\section{Analytics}
\lettrine{S}{eek} and destroy operations based on data.
Outliers, reporting, deviation from the known, user 
\section{Humint}
academics, works with other teams, manage expectation, management buy in
\end{document}
